{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Import files","metadata":{}},{"cell_type":"markdown","source":"# **EDSA Sentiment analysis**","metadata":{}},{"cell_type":"markdown","source":"**by**","metadata":{}},{"cell_type":"markdown","source":"# **Cameron Pike**","metadata":{}},{"cell_type":"markdown","source":"# **Introduction**","metadata":{}},{"cell_type":"markdown","source":"This notebook will provide insight into the factors and opinions surrounding climate change. \nI created a Machine Learning model that is able to classify whether or not a person believes in climate change, based on their tweet data.","metadata":{}},{"cell_type":"code","source":"# utilities\nimport re\nimport numpy as np\nimport pandas as pd\n# plotting\nimport seaborn as sns\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n# nltk\nfrom nltk.stem import WordNetLemmatizer\n# sklearn\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn import model_selection, naive_bayes, svm\n\nimport nltk\nfrom nltk import TreebankWordTokenizer, SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nimport string\nimport urllib\n\nfrom nltk.tokenize import word_tokenize as WordTokenizer\n\n\nnltk.download('wordnet')\nnltk.download('stopwords')\nnltk.download('omw-1.4')\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2022-10-13T07:12:28.675778Z","iopub.execute_input":"2022-10-13T07:12:28.676241Z","iopub.status.idle":"2022-10-13T07:12:31.847277Z","shell.execute_reply.started":"2022-10-13T07:12:28.676143Z","shell.execute_reply":"2022-10-13T07:12:31.846305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Import files**","metadata":{}},{"cell_type":"markdown","source":"Import the files into dataframes","metadata":{}},{"cell_type":"code","source":"\ndf = pd.read_csv('../input/edsa-sentiment-classification/train.csv')\ndf_test = pd.read_csv('../input/edsa-sentiment-classification/test.csv')\nsample = pd.read_csv('../input/edsa-sentiment-classification/sample_submission.csv')\n","metadata":{"execution":{"iopub.status.busy":"2022-10-13T07:12:31.849033Z","iopub.execute_input":"2022-10-13T07:12:31.849595Z","iopub.status.idle":"2022-10-13T07:12:32.063919Z","shell.execute_reply.started":"2022-10-13T07:12:31.849560Z","shell.execute_reply":"2022-10-13T07:12:32.061549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Exploratory Data Analysis**","metadata":{}},{"cell_type":"markdown","source":"View the first 5 rows of the training data","metadata":{}},{"cell_type":"code","source":"print(df.head())","metadata":{"execution":{"iopub.status.busy":"2022-10-13T07:12:38.660377Z","iopub.execute_input":"2022-10-13T07:12:38.660788Z","iopub.status.idle":"2022-10-13T07:12:38.677025Z","shell.execute_reply.started":"2022-10-13T07:12:38.660756Z","shell.execute_reply":"2022-10-13T07:12:38.675393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"View the first 5 rows of the test data","metadata":{}},{"cell_type":"code","source":"print(df_test.head())","metadata":{"execution":{"iopub.status.busy":"2022-10-13T07:13:57.565072Z","iopub.execute_input":"2022-10-13T07:13:57.565537Z","iopub.status.idle":"2022-10-13T07:13:57.575325Z","shell.execute_reply.started":"2022-10-13T07:13:57.565499Z","shell.execute_reply":"2022-10-13T07:13:57.573266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"View the first 5 rows of the sample data","metadata":{}},{"cell_type":"code","source":"print(sample.head())","metadata":{"execution":{"iopub.status.busy":"2022-10-13T07:14:23.495081Z","iopub.execute_input":"2022-10-13T07:14:23.495565Z","iopub.status.idle":"2022-10-13T07:14:23.504448Z","shell.execute_reply.started":"2022-10-13T07:14:23.495529Z","shell.execute_reply":"2022-10-13T07:14:23.503001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Drop the last column of the sample data","metadata":{}},{"cell_type":"code","source":"#Drop sentiment column in sample. We will add the predicted sentiment later\nsample = sample.iloc[: , :-1]\nprint(sample.head())","metadata":{"execution":{"iopub.status.busy":"2022-10-13T07:20:03.622876Z","iopub.execute_input":"2022-10-13T07:20:03.623333Z","iopub.status.idle":"2022-10-13T07:20:03.634235Z","shell.execute_reply.started":"2022-10-13T07:20:03.623299Z","shell.execute_reply":"2022-10-13T07:20:03.632821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Analyse the columns, shape and data types in the training data","metadata":{}},{"cell_type":"code","source":"print(\"Columns in train\")\nprint(df.columns)\n\nprint(\"Shape of train\")\nprint(df.shape)\n\nprint(\"Info of train\")\nprint(df.info())\n\nprint(\"Data types in train\")\nprint(df.dtypes)\n\n#np.sum(df.isnull().any(axis=1))\nprint('Count of columns in the data is:  ', len(df.columns))\nprint('Count of rows in the data is:  ', len(df))\n\n\nprint(\"Unique sentiment values in train\")\nprint(df['sentiment'].unique())\n\n\nprint(\"Total Unique sentiment values in train\")\n#Total uniques\nprint(df['sentiment'].nunique())\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-10-13T07:20:05.890891Z","iopub.execute_input":"2022-10-13T07:20:05.891324Z","iopub.status.idle":"2022-10-13T07:20:05.927266Z","shell.execute_reply.started":"2022-10-13T07:20:05.891289Z","shell.execute_reply":"2022-10-13T07:20:05.926014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Analyse the columns, shape and data types in the test data","metadata":{}},{"cell_type":"code","source":"print(\"Columns in test\")\nprint(df_test.columns)\n\nprint(\"Shape of test\")\nprint(df_test.shape)\n\nprint(\"Info of test\")\nprint(df_test.info())\n\nprint(\"Data types in test\")\nprint(df_test.dtypes)\n\n#np.sum(df.isnull().any(axis=1))\nprint('Count of columns in the data is:  ', len(df_test.columns))\nprint('Count of rows in the data is:  ', len(df_test))\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-10-13T07:22:27.016142Z","iopub.execute_input":"2022-10-13T07:22:27.016562Z","iopub.status.idle":"2022-10-13T07:22:27.035744Z","shell.execute_reply.started":"2022-10-13T07:22:27.016526Z","shell.execute_reply":"2022-10-13T07:22:27.034437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Data preprocessing,","metadata":{}},{"cell_type":"markdown","source":"Place the training data columns into a list","metadata":{}},{"cell_type":"code","source":"# Storing data in lists.\n\nmessage, sentiment = list(df['message']), list(df['sentiment'])\n\ndata=df[['message','sentiment']]\n#print(\"data\")\n#print(data)\ndata_test=df_test[['message']]\n#print(\"data_test\")\n#print(data_test)\n","metadata":{"execution":{"iopub.status.busy":"2022-10-13T07:34:22.920978Z","iopub.execute_input":"2022-10-13T07:34:22.921398Z","iopub.status.idle":"2022-10-13T07:34:22.935784Z","shell.execute_reply.started":"2022-10-13T07:34:22.921366Z","shell.execute_reply":"2022-10-13T07:34:22.934511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Clean the data**","metadata":{}},{"cell_type":"code","source":"\n######################################################################################\n#Clean data\n#####################################################################################\n\n\n#Convert upper case to lower case for training data\ndata['message']=data['message'].str.lower()\nprint(\"before stop words\")\nprint(data['message'].tail())\n\n#Convert upper case to lower case for test data\ndata_test['message']=data_test['message'].str.lower()\nprint(\"before stop words test data\")\nprint(data_test['message'].tail())\n\n\n###############################################\n#Clean punctuation\n###############################################\n\n#Remove punctuation eg !?\n\nenglish_punctuations = string.punctuation\npunctuations_list = english_punctuations\n\n\ndef cleaning_punctuations(message):\n    translator = str.maketrans('', '', punctuations_list)\n    return message.translate(translator)\ndata['message']= data['message'].apply(lambda x: cleaning_punctuations(x))\nprint(\"punctuation\")\nprint(data['message'].tail())\n\ndata_test['message']= data_test['message'].apply(lambda x: cleaning_punctuations(x))\nprint(\"punctuation test data\")\nprint(data_test['message'].tail())\n\n\n\n\n###############################################\n#Clean repeating characters\n###############################################\n\n\ndef cleaning_repeating_char(message):\n    return re.sub(r'(.)1+', r'1', message)\ndata['message'] = data['message'].apply(lambda x: cleaning_repeating_char(x))\nprint(\"after repeating charss\")\nprint(data['message'].tail())\n\ndata_test['message'] = data_test['message'].apply(lambda x: cleaning_repeating_char(x))\nprint(\"after repeating chars test data\")\nprint(data_test['message'].tail())\n\n\n\n###############################################\n#Clean url's\n###############################################\n\n\ndef cleaning_URLs(data):\n    return re.sub('((www.[^s]+)|(https?://[^s]+))',' ',data)\ndata['message'] = data['message'].apply(lambda x: cleaning_URLs(x))\nprint(\"after clean urls\")\nprint(data['message'].tail())\n\ndata_test['message'] = data_test['message'].apply(lambda x: cleaning_URLs(x))\nprint(\"after clean urls test data\")\nprint(data_test['message'].tail())\n\n\n\n###############################################\n#Clean repeating numbers\n###############################################\n\n\ndef cleaning_numbers(data):\n    return re.sub('[0-9]+', '', data)\ndata['message'] = data['message'].apply(lambda x: cleaning_numbers(x))\nprint(\"after clean numbers\")\nprint(data['message'].tail())\n\ndata_test['message'] = data_test['message'].apply(lambda x: cleaning_numbers(x))\nprint(\"after clean numbers test data\")\nprint(data_test['message'].tail())\n\n\n\n###############################################\n#Clean hashtags\n###############################################\n\n\ndef cleaning_hastags(data):\n    return re.sub('#[A-Za-z0-9_]+', '', data)\ndata['message'] = data['message'].apply(lambda x: cleaning_hastags(x))\nprint(\"after clean hastags\")\nprint(data['message'].tail())\n\ndata_test['message'] = data_test['message'].apply(lambda x: cleaning_hastags(x))\nprint(\"after clean hastags test data\")\nprint(data_test['message'].tail())\n\n\n\n###############################################\n#Clean mentions\n###############################################\n\n\ndef cleaning_mentions(data):\n    return re.sub('@[A-Za-z0-9_]+', '', data)\ndata['message'] = data['message'].apply(lambda x: cleaning_mentions(x))\nprint(\"after clean mentions\")\nprint(data['message'].tail())\n\ndata_test['message'] = data_test['message'].apply(lambda x: cleaning_mentions(x))\nprint(\"after clean mentions test data\")\nprint(data_test['message'].tail())\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-10-13T07:34:26.198070Z","iopub.execute_input":"2022-10-13T07:34:26.198481Z","iopub.status.idle":"2022-10-13T07:34:27.109242Z","shell.execute_reply.started":"2022-10-13T07:34:26.198447Z","shell.execute_reply":"2022-10-13T07:34:27.108146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"Tokenization\nWord tokenization is the process of splitting a large sample of text into words. This is a requirement in natural language processing tasks where each word needs to be captured and subjected to further analysis like classifying and counting them for a particular sentiment etc. ","metadata":{}},{"cell_type":"code","source":"##################################################################################################\n#Tokenization\n#Tokenization is the process breaking complex data like paragraphs into simple units called tokens.\n#\n#Sentence tokenization : split a paragraph into list of sentences using sent_tokenize() method\n#Word tokenization : split a sentence into list of words using word_tokenize() method\n#\n#Some other important terms related to word Tokenization are:\n#\n#Bigrams: Tokens consist of two consecutive words known as bigrams.\n#\n#Trigrams: Tokens consist of three consecutive words known as trigrams.\n#\n#Ngrams: Tokens consist of ’N’ number of consecutive words known as ngrams\n#\n#\n###################################################################################################\n\n\ndata['message'] = data['message'].astype(str).apply(WordTokenizer)\nprint(\"after tokenise\")\nprint(data['message'])\n\ndata_test['message'] = data_test['message'].astype(str).apply(WordTokenizer)\nprint(\"after tokenise test\")\nprint(data_test['message'])","metadata":{"execution":{"iopub.status.busy":"2022-10-13T08:22:17.609424Z","iopub.execute_input":"2022-10-13T08:22:17.609874Z","iopub.status.idle":"2022-10-13T08:22:25.501900Z","shell.execute_reply.started":"2022-10-13T08:22:17.609838Z","shell.execute_reply":"2022-10-13T08:22:25.500588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Stopwords\nData Cleaning plays important role in NLP to remove noise from data.\nStopwords : refers to the most common words in a language (such as “the”, “a”, “an”, “in”)\nwhich helps in formation of sentence to make sense, but these words does not provide \nany significance in language processing so remove it .","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"###################################################################################################\n#Stopwords\n#\n#Data Cleaning plays important role in NLP to remove noise from data.\n#Stopwords : refers to the most common words in a language (such as “the”, “a”, “an”, “in”)\n#which helps in formation of sentence to make sense, but these words does not provide \n#any significance in language processing so remove it .\n#\n#You can check list of stopwords by running below code snippet :\n###################################################################################################\n\n\n\nstop_words = set(stopwords.words(\"english\"))\n\ndef cleaning_stopwords(message):  \n    return \" \".join([word for word in str(message).split() if word not in stop_words])\n\ndata['message'] = data['message'].apply(lambda message: cleaning_stopwords(message))\nprint(\"after stop words\")\nprint(data['message'].head(20))\n\ndata_test['message'] = data_test['message'].apply(lambda message: cleaning_stopwords(message))\nprint(\"after stop words test data\")\nprint(data_test['message'].head(20))","metadata":{"execution":{"iopub.status.busy":"2022-10-13T08:22:25.503627Z","iopub.execute_input":"2022-10-13T08:22:25.503959Z","iopub.status.idle":"2022-10-13T08:22:26.013475Z","shell.execute_reply.started":"2022-10-13T08:22:25.503931Z","shell.execute_reply":"2022-10-13T08:22:26.011949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"PorterStemmer\nStemming is the process of producing morphological variants of a root/base word.\nStemming programs are commonly referred to as stemming algorithms or stemmers.\nA stemming algorithm reduces the words “chocolates”, “chocolatey”, and “choco” to the root word, “chocolate”\n","metadata":{}},{"cell_type":"code","source":"\n###################################################################################################\n#PorterStemmer\n#\n#Stemming is the process of producing morphological variants of a root/base word.\n#Stemming programs are commonly referred to as stemming algorithms or stemmers.\n#A stemming algorithm reduces the words “chocolates”, “chocolatey”, and “choco” to the root\n#word, “chocolate” and “retrieval”, “retrieved”, “retrieves” reduce to the stem “retrieve\n###################################################################################################\n\n\nst = nltk.PorterStemmer()\n\ndef stemming_on_text(data):\n    text = [st.stem(word) for word in data]\n    return data\n\ndata['message']= data['message'].apply(lambda x: stemming_on_text(x))\nprint(\"after porter stemmer\")\nprint(data['message'].tail())\n\ndata_test['message']= data_test['message'].apply(lambda x: stemming_on_text(x))\nprint(\"after porter stemmer test data\")\nprint(data_test['message'].tail())\n","metadata":{"execution":{"iopub.status.busy":"2022-10-13T07:34:32.836755Z","iopub.execute_input":"2022-10-13T07:34:32.837195Z","iopub.status.idle":"2022-10-13T07:34:36.554205Z","shell.execute_reply.started":"2022-10-13T07:34:32.837150Z","shell.execute_reply":"2022-10-13T07:34:36.552771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"Lemmatization\nMajor drawback of stemming is it produces Intermediate representation of word. Stemmer may or may not return meaningful word.\n\nTo overcome this problem Lemmatization comes into picture.\n\nStemming algorithm works by cutting suffix or prefix from the word.On the contrary Lemmatization consider morphological analysis of the words and returns meaningful word in proper form.\n\nHence,lemmatization is preferred.","metadata":{}},{"cell_type":"code","source":"\n#######################################################################################################\n#Lemmatization\n#Major drawback of stemming is it produces Intermediate representation of word. Stemmer may or may not return meaningful word.\n#\n#To overcome this problem Lemmatization comes into picture.\n#\n#Stemming algorithm works by cutting suffix or prefix from the word.On the contrary Lemmatization consider morphological analysis of the words and returns meaningful word in proper form.\n#\n#Hence,lemmatization is preferred.\n#########################################################################################################\n\n\nlm = nltk.WordNetLemmatizer()\n\ndef lemmatizer_on_text(data):\n    text = [lm.lemmatize(word) for word in data]\n    return data\n\ndata['message'] = data['message'].apply(lambda x: lemmatizer_on_text(x))\nprint(\"after lemmatizer\")\nprint(data['message'].tail())\n\ndata_test['message'] = data_test['message'].apply(lambda x: lemmatizer_on_text(x))\nprint(\"after lemmatizer test data\")\nprint(data_test['message'].tail())","metadata":{"execution":{"iopub.status.busy":"2022-10-13T08:29:33.519584Z","iopub.execute_input":"2022-10-13T08:29:33.520010Z","iopub.status.idle":"2022-10-13T08:30:14.096753Z","shell.execute_reply.started":"2022-10-13T08:29:33.519979Z","shell.execute_reply":"2022-10-13T08:30:14.095499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Modeling**","metadata":{}},{"cell_type":"markdown","source":"TF-IDF stands for Term Frequency-Inverse Document Frequency\n“Term frequency–inverse document frequency, is a numerical statistic that is intended to \nreflect how important a word is to a document in a collection or corpus.”\n\nTerm Frequency: is a scoring of the frequency of the word in the current document.\nInverse Document Frequency: is a scoring of how rare the word is across documents.\nTF-IDF model contains information on the more important words and the less important ones as well.\n","metadata":{}},{"cell_type":"code","source":"###########################################################################################\n#TF-IDF stands for Term Frequency-Inverse Document Frequency\n#“Term frequency–inverse document frequency, is a numerical statistic that is intended to \n#reflect how important a word is to a document in a collection or corpus.”\n#\n#Term Frequency: is a scoring of the frequency of the word in the current document.\n#Inverse Document Frequency: is a scoring of how rare the word is across documents.\n#TF-IDF model contains information on the more important words and the less important ones as well.\n#\n#Note:In order to use TF-IDF we must pass Strings instead of tokens.\n#we can use detokenizer or join method to join all tokens of list into a single string\n#################################################################################################\n\n\n#Split data into X and Y to create seperate target\nX=data.message\ny=data.sentiment\ncompetion_test=data_test.message\n\n\n\n# Separating the data for training data and for testing data\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.05, random_state =42)\n\n\n\nvectoriser = TfidfVectorizer(ngram_range=(1,2), max_features=500000)\nvectoriser.fit(X_train)\nprint('No. of feature_words: ', len(vectoriser.get_feature_names()))\n\n\nprint('X_train before vectoriser.transform X_train')\nprint(X_train)\n\nX_train = vectoriser.transform(X_train)\nprint('X_train after vectoriser.transform X_train')\nprint(X_train)\n\nX_test  = vectoriser.transform(X_test)\nprint('X_test after vectoriser.transform X_test')\nprint(X_test)\n\n#Our sample from competition to predict on\nX_test_sample  = vectoriser.transform(competion_test)\nprint('X_test_sample after vectoriser.transform data_test')\nprint(X_test_sample)","metadata":{"execution":{"iopub.status.busy":"2022-10-13T08:38:48.482028Z","iopub.execute_input":"2022-10-13T08:38:48.482449Z","iopub.status.idle":"2022-10-13T08:38:51.357211Z","shell.execute_reply.started":"2022-10-13T08:38:48.482414Z","shell.execute_reply":"2022-10-13T08:38:51.356120Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Performance evaluation of models","metadata":{}},{"cell_type":"code","source":"##########################################################################\n#Train and evaluate models\n#################################################################\n\n\ndef model_Evaluate(model):\n # Predict values for Test dataset\n y_pred = model.predict(X_test)\n print(\"y predict\")\n print(y_pred)\n # Print the evaluation metrics for the dataset.\n print(classification_report(y_test, y_pred))\n # Compute and plot the Confusion matrix\n cf_matrix = confusion_matrix(y_test, y_pred)\n\n    \n#BernoulliNB\nBNBmodel = BernoulliNB()\nBNBmodel.fit(X_train, y_train)\nprint(\"score for BNB\")\nmodel_Evaluate(BNBmodel)\n\ny_pred_final_BNB = BNBmodel.predict(X_test_sample)\nprint(\"y predict final BNB\")\nprint(y_pred_final_BNB)\n\n#Add extra column to output\n#Copy the sample file\nsample_BNB_copy = sample.copy()\nsample_BNB_copy['sentiment'] = y_pred_final_BNB\nsample_BNB_copy[['tweetid','sentiment']].to_csv('testsubmission_BNB.csv', index=False)\n\n\n#LinearSVC\nSVCmodel = LinearSVC()\nSVCmodel.fit(X_train, y_train)\nprint(\"score for svc\")\nmodel_Evaluate(SVCmodel)\n\ny_pred_final_SVC = SVCmodel.predict(X_test_sample)\nprint(\"y predict final SVC\")\nprint(y_pred_final_SVC)\n\n#Add extra column to output\n#Copy the sample file\nsample_SVC_copy = sample.copy()\nsample_SVC_copy['sentiment'] = y_pred_final_SVC\nsample_SVC_copy[['tweetid','sentiment']].to_csv('testsubmission_SVC.csv', index=False)\n\n\n\n#Logistic Regression\nLRmodel = LogisticRegression(C = 2, max_iter = 1000, n_jobs=-1)\nLRmodel.fit(X_train, y_train)\nprint(\"score for LRmodel\")\nmodel_Evaluate(LRmodel)\n\ny_pred_final_LR = LRmodel.predict(X_test_sample)\nprint(\"y predict final LR\")\nprint(y_pred_final_LR)\n\n#Add extra column to output\n#Copy the sample file\nsample_LR_copy = sample.copy()\nsample_LR_copy['sentiment'] = y_pred_final_LR\nsample_LR_copy[['tweetid','sentiment']].to_csv('testsubmission_LR.csv', index=False)\n\n\n\n\n#RandomForestClassifier()\nRFCmodel = RandomForestClassifier()\nRFCmodel.fit(X_train, y_train)\nprint(\"score for RFCmodel\")\nmodel_Evaluate(RFCmodel)\n\ny_pred_final_RFC = RFCmodel.predict(X_test_sample)\nprint(\"y predict final RFC\")\nprint(y_pred_final_RFC)\n\n#Add extra column to output\n#Copy the sample file\nsample_RFC_copy = sample.copy()\nsample_RFC_copy['sentiment'] = y_pred_final_RFC\nsample_RFC_copy[['tweetid','sentiment']].to_csv('testsubmission_RFC.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2022-10-13T08:39:02.816932Z","iopub.execute_input":"2022-10-13T08:39:02.817364Z","iopub.status.idle":"2022-10-13T08:40:31.528565Z","shell.execute_reply.started":"2022-10-13T08:39:02.817329Z","shell.execute_reply":"2022-10-13T08:40:31.527074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Visualiztions","metadata":{}},{"cell_type":"code","source":"############################################################################\n#Graphs\n##########################################################################\n\n\nsns.countplot(x='sentiment', data=df)\n\n\n\n#data_neg = data['message'][:500]\ndata_neg = X[:15818]\nplt.figure(figsize = (20,20))\nwc = WordCloud(max_words = 500 , width = 1600 , height = 800,\n               collocations=False).generate(\" \".join(data_neg))\nplt.imshow(wc)\n\n\n\n\n\n# set up label dataframe for future refrences\n'''\nlabel = [-1, 0, 1, 2]\nlabelN = [\"Against\", \"Neutral\", \"Pro\", \"FactualNews\"]\nlabelDesc = [\n    \"the tweet does not believe in man-made climate change\"\n    , \"the tweet neither supports nor refutes the belief of man-made climate change\"\n    , \"the tweet supports the belief of man-made climate change\"\n    , \"the tweet links to factual news about climate change\"\n]\n\ndfLabel = pd.DataFrame(list(zip(label, labelN, labelDesc)), columns=[\"label\", \"sentiment\", \"description\"])\n'''\nprint(df.head())\n\nplt.figure(figsize = (7, 7))\nplt.pie(df.sentiment.value_counts().values, labels = df.sentiment.value_counts().index, autopct = '%2.1f%%', textprops={'fontsize': 15})\nplt.title('Sentiment Distribution of the Tweet Dataset', fontsize=20)\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-10-13T08:44:19.200673Z","iopub.execute_input":"2022-10-13T08:44:19.201141Z","iopub.status.idle":"2022-10-13T08:44:26.255379Z","shell.execute_reply.started":"2022-10-13T08:44:19.201101Z","shell.execute_reply":"2022-10-13T08:44:26.253782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}